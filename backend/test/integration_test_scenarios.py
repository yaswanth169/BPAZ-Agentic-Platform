#!/usr/bin/env python3
"""
Integration Test Scenarios for Docker Export System

This script exercises the real API endpoints and validates the export
system's end-to-end functionality.
"""

import os
import sys
import json
import requests
import time
import tempfile
import zipfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any

# Add the project backend to the Python import path
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

class IntegrationTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            "Content-Type": "application/json",
            "Authorization": "Bearer test-token"
        })
        self.test_results = []
        self.temp_dirs = []
    
    def log_result(self, test_name: str, status: str, message: str = ""):
        """Log the outcome of a single test."""
        print(f"{'‚úÖ' if status == 'PASS' else '‚ùå' if status == 'FAIL' else '‚ÑπÔ∏è'} {test_name}: {status}")
        if message:
            print(f"   ‚Üí {message}")
        
        self.test_results.append({
            "test": test_name,
            "status": status,
            "message": message
        })
    
    def test_dynamic_analyzer_direct(self):
        """Direct dynamic analyzer test"""
        self.log_result("Dynamic Analyzer Direct Test", "INFO", "Testing analyzer directly...")
        
        try:
            from app.core.dynamic_node_analyzer import DynamicNodeAnalyzer
            from app.core.node_registry import NodeRegistry
            
            # Initialize components
            registry = NodeRegistry()
            analyzer = DynamicNodeAnalyzer(registry)
            
            # Test workflow
            test_nodes = [
                {
                    "id": "openai_1",
                    "type": "OpenAIChat",
                    "data": {
                        "model_name": "gpt-4o-mini",
                        "api_key": "sk-test123"
                    }
                }
            ]
            
            # Analyze workflow
            result = analyzer.analyze_workflow(test_nodes)
            
            # Validate result
            if not result:
                self.log_result("Dynamic Analyzer Direct", "FAIL", "Analyzer returned None")
                return False
            
            if not result.required_env_vars:
                self.log_result("Dynamic Analyzer Direct", "FAIL", "No environment variables detected")
                return False
            
            # Check for expected OpenAI API key
            openai_key_found = any(
                "OPENAI" in var.name and "API_KEY" in var.name 
                for var in result.required_env_vars
            )
            
            if not openai_key_found:
                self.log_result("Dynamic Analyzer Direct", "FAIL", "OpenAI API key not detected")
                return False
            
            self.log_result("Dynamic Analyzer Direct", "PASS", 
                          f"Detected {len(result.required_env_vars)} env vars, {len(result.python_packages)} packages")
            return True
            
        except Exception as e:
            self.log_result("Dynamic Analyzer Direct", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_export_routes_integration(self):
        """Test export routes functionality"""
        self.log_result("Export Routes Integration", "INFO", "Testing export routes...")
        
        try:
            from app.routes.export import analyze_workflow_dependencies
            
            # Test workflow data
            test_nodes = [
                {
                    "id": "openai_1",
                    "type": "OpenAIChat",
                    "data": {
                        "model_name": "gpt-4o-mini",
                        "api_key": "sk-test123"
                    }
                },
                {
                    "id": "memory_1",
                    "type": "BufferMemory",
                    "data": {
                        "memory_key": "chat_history"
                    }
                }
            ]
            
            # Call the function directly
            dependencies = analyze_workflow_dependencies(test_nodes)
            
            # Validate response
            if not dependencies:
                self.log_result("Export Routes Integration", "FAIL", "No dependencies returned")
                return False
            
            # Check required fields
            required_fields = ["nodes", "required_env_vars", "python_packages"]
            for field in required_fields:
                if not hasattr(dependencies, field):
                    self.log_result("Export Routes Integration", "FAIL", f"Missing field: {field}")
                    return False
            
            # Validate nodes
            if "OpenAIChat" not in dependencies.nodes:
                self.log_result("Export Routes Integration", "FAIL", "OpenAIChat node not detected")
                return False
            
            # Validate environment variables
            env_var_names = [var.name for var in dependencies.required_env_vars]
            if not any("OPENAI" in name and "API_KEY" in name for name in env_var_names):
                self.log_result("Export Routes Integration", "FAIL", "OpenAI API key not in env vars")
                return False
            
            # Validate packages
            if not any("openai" in pkg.lower() for pkg in dependencies.python_packages):
                self.log_result("Export Routes Integration", "FAIL", "OpenAI package not in dependencies")
                return False
            
            self.log_result("Export Routes Integration", "PASS", 
                          f"Successfully analyzed {len(dependencies.nodes)} nodes")
            return True
            
        except Exception as e:
            self.log_result("Export Routes Integration", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_node_registry_discovery(self):
        """Test node registry node discovery"""
        self.log_result("Node Registry Discovery", "INFO", "Testing node discovery...")
        
        try:
            from app.core.node_registry import NodeRegistry
            
            registry = NodeRegistry()
            registry.discover_nodes()
            
            # Check if some expected nodes are discovered
            expected_nodes = ["OpenAIChat", "BufferMemory", "TavilySearch"]
            available_nodes = list(registry.nodes.keys())
            
            found_nodes = [node for node in expected_nodes if node in available_nodes]
            
            if len(found_nodes) < 2:  # At least 2 expected nodes should be present
                self.log_result("Node Registry Discovery", "FAIL", 
                              f"Only found {len(found_nodes)} expected nodes: {found_nodes}")
                return False
            
            self.log_result("Node Registry Discovery", "PASS", 
                          f"Discovered {len(available_nodes)} total nodes, {len(found_nodes)} expected nodes")
            return True
            
        except Exception as e:
            self.log_result("Node Registry Discovery", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_package_generation_logic(self):
        """Test package dependency generation logic"""
        self.log_result("Package Generation Logic", "INFO", "Testing package generation...")
        
        try:
            from app.core.dynamic_node_analyzer import PackageAnalyzer
            
            analyzer = PackageAnalyzer()
            
            # Test different node types
            test_cases = [
                ("OpenAIChat", ["openai", "langchain-openai"]),
                ("TavilySearch", ["langchain-tavily"]),
                ("BufferMemory", ["langchain-core"])
            ]
            
            for node_type, expected_packages in test_cases:
                packages = analyzer.get_node_packages(node_type)
                
                # Check if at least one expected package is present
                found_expected = any(
                    any(expected in pkg for pkg in packages)
                    for expected in expected_packages
                )
                
                if not found_expected:
                    self.log_result("Package Generation Logic", "FAIL", 
                                  f"No expected packages found for {node_type}")
                    return False
            
            self.log_result("Package Generation Logic", "PASS", 
                          "All node types generated appropriate packages")
            return True
            
        except Exception as e:
            self.log_result("Package Generation Logic", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_credential_detection_logic(self):
        """Test credential detection logic"""
        self.log_result("Credential Detection", "INFO", "Testing credential detection...")
        
        try:
            from app.core.dynamic_node_analyzer import CredentialDetector
            
            detector = CredentialDetector()
            
            # Test credential detection
            test_data = {
                "api_key": "sk-test123",
                "model_name": "gpt-4o-mini",
                "temperature": 0.7,
                "database_url": "postgresql://user:pass@host/db"
            }
            
            credentials = detector.detect_credentials(test_data, "TestNode")
            
            # Should detect api_key and database_url as credentials
            cred_names = [cred.name for cred in credentials]
            
            expected_credentials = ["TESTNODE_API_KEY", "TESTNODE_DATABASE_URL"]
            found_credentials = [name for name in expected_credentials if name in cred_names]
            
            if len(found_credentials) < 2:
                self.log_result("Credential Detection", "FAIL", 
                              f"Only detected {len(found_credentials)} credentials: {found_credentials}")
                return False
            
            self.log_result("Credential Detection", "PASS", 
                          f"Detected {len(credentials)} credentials correctly")
            return True
            
        except Exception as e:
            self.log_result("Credential Detection", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_fallback_system_behavior(self):
        """Test fallback system"""
        self.log_result("Fallback System", "INFO", "Testing fallback behavior...")
        
        try:
            from app.routes.export import _fallback_static_analysis
            
            # Test fallback with minimal node data
            test_nodes = [
                {
                    "id": "unknown_1", 
                    "type": "UnknownNodeType",
                    "data": {}
                }
            ]
            
            # This should trigger fallback
            dependencies = _fallback_static_analysis(test_nodes)
            
            # Validate fallback response
            if not dependencies:
                self.log_result("Fallback System", "FAIL", "Fallback returned None")
                return False
            
            # Should have basic system dependencies
            if not dependencies.python_packages:
                self.log_result("Fallback System", "FAIL", "No packages in fallback")
                return False
            
            # Should have basic environment variables
            if not dependencies.required_env_vars:
                self.log_result("Fallback System", "FAIL", "No env vars in fallback")
                return False
            
            self.log_result("Fallback System", "PASS", 
                          "Fallback system working correctly")
            return True
            
        except Exception as e:
            self.log_result("Fallback System", "FAIL", f"Exception: {str(e)}")
            return False
    
    def test_end_to_end_workflow(self):
        """End-to-end workflow test"""
        self.log_result("End-to-End Workflow", "INFO", "Testing complete workflow...")
        
        try:
            # Import required modules
            from app.core.node_registry import NodeRegistry
            from app.core.dynamic_node_analyzer import DynamicNodeAnalyzer
            from app.routes.export import analyze_workflow_dependencies
            
            # Full workflow test
            test_workflow = [
                {
                    "id": "openai_1",
                    "type": "OpenAIChat", 
                    "data": {
                        "model_name": "gpt-4o-mini",
                        "api_key": "sk-test123",
                        "temperature": 0.7
                    }
                },
                {
                    "id": "memory_1",
                    "type": "BufferMemory",
                    "data": {
                        "memory_key": "chat_history"
                    }
                }
            ]
            
            # Step 1: Node registry
            registry = NodeRegistry()
            registry.discover_nodes()
            
            # Step 2: Dynamic analysis
            analyzer = DynamicNodeAnalyzer(registry)
            analysis_result = analyzer.analyze_workflow(test_workflow)
            
            # Step 3: Export route processing
            dependencies = analyze_workflow_dependencies(test_workflow)
            
            # Validate complete flow
            if not analysis_result or not dependencies:
                self.log_result("End-to-End Workflow", "FAIL", "One component failed")
                return False
            
            # Check consistency between analysis and dependencies
            analyzer_nodes = len(analysis_result.node_types)
            dependencies_nodes = len(dependencies.nodes)
            
            if analyzer_nodes != dependencies_nodes:
                self.log_result("End-to-End Workflow", "FAIL", 
                              f"Node count mismatch: {analyzer_nodes} vs {dependencies_nodes}")
                return False
            
            self.log_result("End-to-End Workflow", "PASS", 
                          "Complete workflow executed successfully")
            return True
            
        except Exception as e:
            self.log_result("End-to-End Workflow", "FAIL", f"Exception: {str(e)}")
            return False
    
    def cleanup(self):
        """Cleanup temporary directories"""
        for temp_dir in self.temp_dirs:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
    
    def run_all_tests(self):
        """Run all integration tests"""
        print("üß™ DOCKER EXPORT INTEGRATION TESTS")
        print("=" * 40)
        
        tests = [
            self.test_node_registry_discovery,
            self.test_dynamic_analyzer_direct,
            self.test_credential_detection_logic,
            self.test_package_generation_logic,
            self.test_export_routes_integration,
            self.test_fallback_system_behavior,
            self.test_end_to_end_workflow
        ]
        
        for test in tests:
            try:
                test()
                time.sleep(0.2)  # Brief pause
            except Exception as e:
                self.log_result(test.__name__, "FAIL", f"Unexpected error: {str(e)}")
        
        # Summary
        total = len(self.test_results)
        passed = len([r for r in self.test_results if r["status"] == "PASS"])
        failed = len([r for r in self.test_results if r["status"] == "FAIL"])
        
        print(f"\nüìä TEST SUMMARY")
        print(f"Total: {total}, Passed: {passed}, Failed: {failed}")
        print(f"Success Rate: {(passed/total*100):.1f}%")
        
        if failed > 0:
            print(f"\n‚ùå FAILED TESTS:")
            for result in self.test_results:
                if result["status"] == "FAIL":
                    print(f"   ‚Ä¢ {result['test']}: {result['message']}")
        
        return passed, failed, total

def main():
    """Main test runner"""
    tester = IntegrationTester()
    
    try:
        passed, failed, total = tester.run_all_tests()
        
        if failed == 0:
            print(f"\nüéâ ALL TESTS PASSED! System ready for deployment.")
            return 0
        else:
            print(f"\n‚ö†Ô∏è {failed} test(s) failed. Review issues before deployment.")
            return 1
            
    except Exception as e:
        print(f"\nüí• Test suite crashed: {str(e)}")
        return 2
    finally:
        tester.cleanup()

if __name__ == "__main__":
    exit(main())